# Eye-For-The-Blind_AryaSwain
The primary goal was to develop a deep learning model capable of generating descriptive captions for images, empowering visually impaired individuals to perceive visual content through auditory means.

The World Health Organization (WHO) has reported that approximately 285 million people are visually impaired worldwide, and out of these 285 million, 39 million are completely blind. It gets extremely tough for them to carry out daily activities, one of which is reading. From reading a newspaper or a magazine to reading an important text message from your bank, it is tough for them to read the text written in it.

A similar problem they also face is seeing and enjoying the beauty of pictures and images. Today, in the world of social media, millions of images are uploaded daily. Some of them are about your friends and family, while some of them are about nature and its beauty. Understanding what is present in that image is quite a challenge for certain people who are suffering from visual impairment or who are blind.

In an initiative to help them experience the beauty of the images, Facebook launched a unique feature earlier that can help blind people operate the app on their mobile phones. The feature could explain the contents of an image that their friends have posted on Facebook. So, say, if someone posted a picture with their dog in the park, the application would speak out the contents and may describe it like, “This image may contain a dog standing with a man around the trees.”

This model aims to address the challenges faced by visually impaired individuals by enabling them to comprehend visual content through a CNN-RNN-based approach. It focuses on developing a system where blind individuals can understand the contents of an image in their surroundings, thus empowering them to engage fully in various activities. The model will convert the contents of an image and will provide the output in the form of audio.
